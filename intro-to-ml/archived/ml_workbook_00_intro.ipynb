{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ip6kH_LkjiS"
   },
   "source": [
    "# Part One: Introduction & Classification with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZP67XlnkjiU"
   },
   "source": [
    "## Outline\n",
    "\n",
    "**Main Goal:** Introduce the machine learning pipeline in scikit-learn\n",
    "\n",
    "- Define Jupyter Notebooks\n",
    "- Learn what machine learning is\n",
    "- Represent data in `scikit-learn`\n",
    "- Classify data with the `iris` data\n",
    "- Modify and extend a classification pipeline to new models and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Notebooks in Google Colab\n",
    "\n",
    "This lesson is composed as a Jupyter Notebook. A Jupyter Notebook provides a computational environment in which to run, write, and edit code snippets alongside text and other content in Markdown format.\n",
    "\n",
    "You have a number of options for how to run Jupyter Notebooks. For this workshop, we will use the cloud-based computational environment *Google Colaboratory* (or *Google Colab*). To execute code in a cell, select it,  then either press the play button to the left of the code or use the keyboard shortcut `Command/Ctrl+Enter`. To edit code, just click the cell and start editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lTI3-7QkjiW"
   },
   "source": [
    "## About Scikit-Learn\n",
    "\n",
    "[`Scikit-Learn`](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to **well-known** machine learning algorithms within Python code through a **clean, well-thought-out API**. Hundreds of contributors from around the world built it and it is used across industry and academia.\n",
    "\n",
    "`Scikit-Learn` is built upon Python's [`NumPy` (Numerical Python)](http://numpy.org) and [`SciPy` (Scientific Python)](http://scipy.org) libraries, which enable efficient in-core numerical and scientific computation within Python. Other popular ML libraries in Python include [`TensorFlow`](https://www.tensorflow.org/) and [`PyTorch`](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itYkBcy4kjiZ"
   },
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "In this section we will begin to explore the basic principles of machine learning.\n",
    "Machine Learning is a familiy of techniques that derive patterns from observed data and apply those patterns to previously unobserved data. \n",
    "\n",
    "Machine learning is a subfield of **Artificial Intelligence** in that algorithms are building blocks to make computers learn and behave more intelligently by (somehow) **generalizing** rather that just storing and retrieving data items like a database system would do.\n",
    "\n",
    "In contrast to statistical methods that prioritize models and statistical inference, machine learning takes a **data-first** approach that prioritizes **prediction**. One common flow in working with machine learning is to \n",
    "\n",
    "1. Gather and observe a collection of data that is representative of the world\n",
    "2. Abstract out patterns, rules, and association between a collection of observations (or \"features\") and a desired \"target\" variable, and \n",
    "3. Predict the target variable in previously unseen collection of data, given those features.\n",
    "\n",
    "To get started, we'll focus on one subset of machine learning approaches: **classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQxmeI7wkjib"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7J296gykjix"
   },
   "source": [
    "## Representation of Data in Scikit-learn\n",
    "\n",
    "Machine learning is about creating models from data; therefore we'll start by\n",
    "discussing how to represent data in a way the computer understands.  Along\n",
    "with this, we'll create `matplotlib` examples to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqKTTohMkjiy"
   },
   "source": [
    "Most machine learning algorithms implemented in `scikit-learn` expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **`n_samples`.**   The number of samples: Each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **`n_features`.**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "You should fix the number of features in advance. The number can be very high\n",
    "(e.g., with millions of features in a \"high dimensional\" data set), perhaps even with most data points being zeros. \n",
    "`scipy.sparse` matrices can be useful if you have a lot of zeroes: They are\n",
    "more memory-efficient than `numpy` arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVd-6y5fkji0"
   },
   "source": [
    "![Data Layout](https://github.com/jakevdp/sklearn_tutorial/blob/master/notebooks/images/data-layout.png?raw=1)\n",
    "\n",
    "(Figure from the [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZxVz_6mkji1"
   },
   "source": [
    "## Example: the Iris Dataset\n",
    "\n",
    "We're going to use the `iris` data bundled with `scikit-learn` (Anderson, 1936; Fisher, 1936). The data consists of measurements of the three different species of iris shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "7xID93r9kji2",
    "outputId": "7ed068b8-b986-4b39-d036-5b94fe160749"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "Image(url=\"https://raw.githubusercontent.com/jakevdp/sklearn_tutorial/master/notebooks/images/iris_setosa.jpg\")\n",
    "# Iris Setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "9BQSSQiTBxNT",
    "outputId": "f9c48d60-341a-4995-fc6b-1fec403f9f28"
   },
   "outputs": [],
   "source": [
    "Image(url = 'http://raw.githubusercontent.com/jakevdp/sklearn_tutorial/master/notebooks/images/iris_versicolor.jpg')\n",
    "# Iris Versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "P1JXzYSxB507",
    "outputId": "569131d3-8c45-4d8f-9ff5-6868ca6b8417"
   },
   "outputs": [],
   "source": [
    "Image(url='http://raw.githubusercontent.com/jakevdp/sklearn_tutorial/master/notebooks/images/iris_virginica.jpg')\n",
    "# print(\"Iris Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89Os4w0bkji7"
   },
   "source": [
    "### Quick Question:\n",
    "\n",
    "**What kind of data does an algorithm require to recognize iris species?**\n",
    "\n",
    "Recall that we need a 2D array of size `[n_samples x n_features]`.\n",
    "\n",
    "- What are the `n_samples`?\n",
    "\n",
    "- What are the `n_features`?\n",
    "\n",
    "Each sample requires a **fixed** number of features, and feature\n",
    "number ``i`` must be a similar kind of quantity for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4zasqmZkji8"
   },
   "source": [
    "### The `iris` Data\n",
    "\n",
    "The `iris` data is straightforward, with these variables:\n",
    "\n",
    "  - sepal length in cm\n",
    "  - sepal width in cm\n",
    "  - petal length in cm\n",
    "  - petal width in cm\n",
    "\n",
    "Target classes to predict are\n",
    "\n",
    "  - *Iris Setosa*\n",
    "  - *Iris Versicolour*\n",
    "  - *Iris Virginica*\n",
    "  \n",
    "Use a `Scikit-learn` helper function to load the `iris` data into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aU0WDPdkji9"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vKZbtI_MkjjB",
    "outputId": "dac9c84d-0e00-444e-84bf-7c953ac90368"
   },
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the four feature variables\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add a new column with the species names, this is what we are going to try to predict\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "\n",
    "# View the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2XWt0TOLkjjF",
    "outputId": "b1e8a67a-53de-45dc-bedd-394beeeb839e"
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print((n_samples, n_features))\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bnqA-fHVkjjJ",
    "outputId": "cb18f4a0-429b-471b-b772-5ee97012d9e7"
   },
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5MIG2ksFkjjL",
    "outputId": "ccfed7cd-c88c-4d00-b3ea-744ae8514b57"
   },
   "outputs": [],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3UmPdufDkjjO",
    "outputId": "a974d484-0b89-4b9a-999e-db25f08d6197"
   },
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSvEc2ibkjjS"
   },
   "source": [
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a simple scatter-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "_hnWDgLJkjjT",
    "outputId": "9de3e032-8929-48f5-c198-720ff2682de7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_index = 0\n",
    "y_index = 1\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z50D1jsJkjjV"
   },
   "source": [
    "### Quick Exercise:\n",
    "\n",
    "**Change** `x_index` **and** `y_index` **in the above script\n",
    "and find a combination of two parameters\n",
    "which maximally separates the three classes.**\n",
    "\n",
    "This exercise is a preview of **dimensionality reduction**, which we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started with classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Classification and Regression\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of both features and labels.\n",
    "The task is to construct an estimator which is able to predict the label of an object\n",
    "given the set of features. A relatively simple example is predicting the species of \n",
    "iris given a set of measurements of its flower. Here are some more complicated examples:\n",
    "\n",
    "- given a multicolor image of an object, determine whether that object is a star, a quasar, or a galaxy\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like\n",
    "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
    "\n",
    "What these tasks have in common is that there is one or more unknown\n",
    "quantities associated with the object which needs to be determined from other\n",
    "observed quantities.\n",
    "\n",
    "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
    "In classification, the label is discrete, while in regression, the label is continuous. For example,\n",
    "in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a\n",
    "classification problem: the label is from three distinct categories. On the other hand, we might\n",
    "wish to estimate the age of an object based on such observations: this would be a regression problem,\n",
    "because the label (age) is a continuous quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Example\n",
    "K Nearest Neighbors (kNN) is one of the simplest and most useful learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training data with two classes, and a new unknown observation\n",
    "Image(url=\"https://cdn-images-1.medium.com/max/800/0*uNbO79MrS7jvY4qp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors with k = 3\n",
    "Image(url='https://cdn-images-1.medium.com/max/800/0*QmLAPLYUDcpJYwvo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on our iris classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "# load our data\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a features matrix and target array\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate the model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "# call the \"predict\" method:\n",
    "\n",
    "sample_data = [[3, 5, 4, 2],]\n",
    "\n",
    "result = knn.predict(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the predicted label\n",
    "\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, predicting a label will not be sufficient. We may also wish to know the confidence the model places in that prediction - or in other words, associate a probability with the each possible target labels.\n",
    "\n",
    "Many models in scikit-learn allow for the `.predict_proba()` method. Like `.predict()`, `.predict_proba()` accepts a set of observatrions as input. In this case, `.predict_proba()` also returns an array of probabilities assigned to each label. Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Your turn: predict a label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data_2 = YOUR_DATA_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_2 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iris.target_names[result_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Scikit-learn's estimator interface\n",
    "\n",
    "The `Scikit-learn` developers have tried to maintain a uniform user interface across all methods. We'll see examples of these below. Given a `scikit-learn` *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.predict()` : predict labels in clustering algorithms.\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data.\n",
    "    \n",
    "- Available in **some estimators**\n",
    "  + `model.predict_proba()`: return probabilities of class predictions, ordered by class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "An important piece of machine learning is **model validation**: that is, determining how well your model will generalize from the training data to future unlabeled data. Let's look at an example using the *nearest neighbor classifier*. This is a very simple classifier: it simply stores all training data, and for any unknown quantity, simply returns the label of the closest training point.\n",
    "\n",
    "With the iris data, it very easily returns the correct prediction for each of the input points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-294ce8c5bb10>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-294ce8c5bb10>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print(y!=y_pred).all())\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = iris.data, iris.target\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(y!=y_pred).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more useful way to look at the results is to view the **confusion matrix**, or the matrix showing the frequency of inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, all 50 training samples are correctly identified. But this **does not mean that our model is perfect!** In particular, such a model generalizes extremely poorly to new data. We can simulate this by splitting our data into a *training set* and a *testing set*. `scikit-learn` contains some convenient routines to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "\n",
    "print(Xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "So far, we have been working towards the goal of improving the performance of our classifer model. In a research context, we may actually be more interested in the **features** in the model than the task of **prediction** itself. That is, we want to identify features that are most predictive of our target oucome.\n",
    "\n",
    "The performance of our estimator is still quite important; it gives us confidence we have learned a relationship between `X` and `y`. But we may also require more insight into the importance of features.\n",
    "\n",
    "Let's take an example from medical research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://upload.wikimedia.org/wikipedia/commons/8/8b/Breast_fibroadenoma_by_fine_needle_aspiration_%282%29_PAP_stain.jpg', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be using the **Wisconsin Diagnostic Breast Cancer (WDBC)** dataset. More information [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).\n",
    "\n",
    "The original repository) contains several versions of the breast cancer data. For today we're interested in Dr. Wolberg's original encodings of the FNA image data, which uses a ranking of 1 to 10 to describe a variety of attributes such unifority of cell shape and size, mitoses and normal nuclei count, margin adhesion, clump thickness, and so on.\n",
    "\n",
    "Let's say we start with the follwing research question:\n",
    "\n",
    "> Can we predict with some accuracy the malignancy of a breast growth based on clinical observations of fine needle aspiration (FNA) images? If so, what specific observations or measurements are most predictive of malignancy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: How would we approach this as a classification question?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our data, train our classifier, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_data = pd.read_csv('https://raw.githubusercontent.com/arcus/education-materials/master/intro-to-ml/wisconsin_data_clean.csv')\n",
    "wi_data = wi_data.drop(wi_data.columns[0], axis=1)\n",
    "wi_data = wi_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the convention of capital X for features matrix\n",
    "# and lowercase y for target array\n",
    "X_wi = wi_data.drop(columns=['sample_code', 'is_benign', 'is_malignant'])\n",
    "y_wi = wi_data['is_malignant']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_wi, y_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(Xtrain, ytrain)\n",
    "\n",
    "print(rf.score(Xtrain, ytrain))\n",
    "print(rf.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: Why are these two scores different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = rf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_wi.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option One: Run classification on the iris dataset using a Random Forest Classifier AND a test/train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) Import your data (and take a look at shape + metadata, for your reference!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load our data again\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "## create feature matrix and target array based on the dataset\n",
    "## where did you store the dataset after loading it?\n",
    "\n",
    "# X, y = ??.data, ??.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Set train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain, Xtest, ytrain, ytest = train_test_split(???, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Instantiate your model as an object (and, optionally, set any parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf.??(???, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Apply your model (make predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new_sample = [[??, ??, ??, ??],]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Validate on test data (and, for your reference, compare to performance on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score on train\n",
    "svm.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Compare feature importances in your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_wi.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option Two: Try classification with a new dataset: blood donation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are going to use a new datase from **openml** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) Import your data (and take a look at shape + metadata, for your reference!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "blood = fetch_openml('blood-transfusion-service-center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blood.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Set train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rewrite this to set train and test splits\n",
    "## following naming conventions from elsewhere in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape\n",
    "# y_train.shape\n",
    "# X_test.shape\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Instantiate your model as an object (and, optionally, set any parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL OF YOUR CHOICE!\n",
    "## One option: LinearSVC\n",
    "\n",
    "# from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Apply your model (make predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Validate on test data (and, for your reference, compare to performance on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you first and foremost to my fantastic reviewers: Victor Ruiz, Sheila Braun, and Remo Williams.\n",
    "    \n",
    "These notebooks are based primarily on two sources:\n",
    "\n",
    "1. [Jake Vanderplas' Scikit-learn tutorial](https://github.com/jakevdp/sklearn_tutorial/) \n",
    "View [license](https://github.com/jakevdp/sklearn_tutorial/blob/master/LICENSE)\n",
    "2. [Andreas Müller's Introduction to Machine learning with Python workshop](https://github.com/amueller/ml-workshop-1-of-4) \n",
    "View [license](https://github.com/amueller/ml-workshop-1-of-4/blob/master/LICENSE)\n",
    "\n",
    "This notebook is a project of [Arcus Education](https://education.arcus.chop.edu/). The primary author is [Zoë Wilkinson Saldaña](https://github.com/zoews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 02.1-Machine-Learning-Intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
